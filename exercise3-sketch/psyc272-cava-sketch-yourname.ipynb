{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sketch Understanding exercise\n",
    "\n",
    "Complete and submit this completed worksheet, including its outputs and any supporting code outside of the worksheet.\n",
    "\n",
    "In this exercise you will:\n",
    "\n",
    "- work with high-level feature representations of photos and sketches extracted from a modern CNN pre-trained on image categorization on the ImageNet photo dataset (VGG-19; Simonyan & Zisserman, 2014). \n",
    "- matched photo-sketch dataset from: _Sangkloy, P., Burnell, N., Ham, C., & Hays, J. (2016). The sketchy database: learning to retrieve badly drawn bunnies. ACM Transactions on Graphics (TOG), 35(4), 119._\n",
    "- compute a \"Representational Dissimilarity Matrix\" (RDM; Kriegeskorte, 2008) for each image domain (i.e., photos, sketches) using features from different layers of the CNN\n",
    "- apply clustering to find groups of visually similar object categories\n",
    "- compare RDMs between image domains for different layers\n",
    "- practice using commonly used methods from `sklearn`/`scipy`\n",
    "- practice using `pandas` to manipulate dataframes and designing your own custom functions to analyze high-dimensional data\n",
    "\n",
    "Note that the dataset for this exercise is around 3GB in size. You may experience some slowdowns when loading and working with datasets of this size in a jupyter notebook running on your local machine. If these slowdowns are prohibitive, please know that this course has access to shared machine learning & data science compute resources through: https://datahub.ucsd.edu. If you are thinking about using the datahub resources, let the instructor know, so we can troubleshoot any issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## general\n",
    "import numpy as np\n",
    "import os, sys\n",
    "import pandas as pd\n",
    "\n",
    "## plotting\n",
    "import  matplotlib\n",
    "from matplotlib import pylab, mlab, pyplot\n",
    "%matplotlib inline\n",
    "from IPython.core.pylabtools import figsize, getfigs\n",
    "plt = pyplot\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['pdf.fonttype'] = 42\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_context('talk')\n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "## sklearn\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn import svm\n",
    "from sklearn import linear_model, datasets, neighbors\n",
    "from sklearn.cluster import AffinityPropagation, KMeans\n",
    "from sklearn import metrics\n",
    "\n",
    "## scipy\n",
    "import scipy.spatial.distance as dist\n",
    "from scipy import stats\n",
    "\n",
    "## UI nice thing\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## directory & file hierarchy\n",
    "proj_dir = os.path.abspath(os.getcwd())\n",
    "feature_dir = os.path.join(proj_dir,'features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(X):\n",
    "    '''\n",
    "    z-score normalization to center and re-scale embeddings\n",
    "    '''\n",
    "    X = X - X.mean(0)\n",
    "    X = X / np.maximum(X.std(0), 1e-5)\n",
    "    return X\n",
    "\n",
    "def flatten(x):\n",
    "    '''\n",
    "    flatten a list of lists\n",
    "    '''\n",
    "    return [item for sublist in x for item in sublist]\n",
    "\n",
    "def preprocess_meta(M):\n",
    "    '''\n",
    "    input: pandas dataframe with a column named 'path'\n",
    "    output: copy of pandas dataframe with additional 'category' and 'fname' columns, parsed from 'path'\n",
    "            'fname' stands for filename\n",
    "    '''    \n",
    "    #############################################################################\n",
    "    # TODO: Fill in this function according to docstring.                       #    \n",
    "    #############################################################################\n",
    "        \n",
    "    return M\n",
    "\n",
    "def load_features(feature_dir,\n",
    "                  layer_name='FC6',\n",
    "                  data_type='photo',\n",
    "                  normed=True):\n",
    "    '''\n",
    "    load in features (.npy) and metadata (.csv) for particular layer of VGG\n",
    "    data type: 'photo' or 'sketch'\n",
    "    normed: boolean indicating whether to z-score features within feature dimension\n",
    "    '''\n",
    "    F = np.load(os.path.join(feature_dir,'FEATURES_VGG_{}_{}.npy'.format(layer_name,data_type)))\n",
    "    M = pd.read_csv(os.path.join(feature_dir,'METADATA_{}.csv'.format(data_type)))\n",
    "    M = preprocess_meta(M)\n",
    "    if normed:\n",
    "        F = normalize(F)\n",
    "    return F, M\n",
    "\n",
    "def extract_features_only(DF):\n",
    "    '''\n",
    "    input: dataframe with both feature indices and metadata columns\n",
    "    output: dataframe with only feature indices (numerical)\n",
    "    '''\n",
    "    num_feats = len([i for i in list(DF.columns) if type(i) is not str]) ## only numeric columns extracted\n",
    "    return DF[list(np.arange(num_feats))]\n",
    "\n",
    "def visualize_matrix(D,obj_list=None):\n",
    "    '''\n",
    "    generate visualization of custom distance matrix\n",
    "    '''\n",
    "    fig = plt.figure(figsize(16,16))\n",
    "    sns.set_style('dark')\n",
    "    plt.matshow(D,cmap='viridis')    \n",
    "    \n",
    "    ## plot params\n",
    "    plt.colorbar(fraction=0.045)\n",
    "    plt.tick_params(\n",
    "        axis='x',          # changes apply to the x-axis\n",
    "        which='both',      # both major and minor ticks are affected\n",
    "        bottom=False,      # ticks along the bottom edge are off\n",
    "        top=False,         # ticks along the top edge are off\n",
    "        labelbottom=False) # labels along the bottom edge are off    \n",
    "    \n",
    "    ## add object labels, if passed to func\n",
    "    if obj_list is not None:\n",
    "        plt.xticks(range(len(D)), obj_list, fontsize=9,rotation='vertical')\n",
    "        plt.yticks(range(len(D)), obj_list, fontsize=9)  \n",
    "        \n",
    "def apply_clustering(DF, n_clusters=4):\n",
    "    '''\n",
    "    apply Kmeans clustering to feature vectors and add cluster indices to dataframe\n",
    "    '''\n",
    "    F = extract_features_only(DF)\n",
    "    #############################################################################\n",
    "    # TODO: Apply KMeans clustering with n_clusters, then add new column to DF  #\n",
    "    # called `cluster_inds` that contains the cluster indices.                  #\n",
    "    #############################################################################    \n",
    "    return DF\n",
    "\n",
    "def get_common_cluster_inds(Pmean,Smean, n_clusters=14):\n",
    "    '''\n",
    "    input: class-mean feature representation dataframes for photos (Pmean) and sketches (Smean) \n",
    "    purpose: apply clustering to photo feature matrix, and use to get common cluster indices that are then \n",
    "             added to both Pmean and Smean\n",
    "    output: Pmean and Smean with additional column named 'common_cluster_inds'\n",
    "    '''\n",
    "    _Pmean = apply_clustering(Pmean, n_clusters=n_clusters)\n",
    "    #############################################################################\n",
    "    # TODO: Fill in this function according to docstring.                       #    \n",
    "    #############################################################################    \n",
    "    \n",
    "    return Pmean, Smean                        \n",
    "\n",
    "def get_ordered_distance_matrix(DF, \n",
    "                                metric='correlation', \n",
    "                                viz=True):\n",
    "    '''\n",
    "    input:\n",
    "        DF is a dataframe containing feature columns and metadata\n",
    "        metric: pick a distance metric from options available from scipy.spatial.distance\n",
    "                e.g., correlation, cosine, cityblock, euclidean\n",
    "        viz is boolean flag to control whether we visualize matrix or not\n",
    "    '''\n",
    "    #############################################################################\n",
    "    # TODO: Fill in this function. Compute distance matrix using pdist and      #\n",
    "    # squareform from scipy.spatial.distance. Make sure that categories are     #    \n",
    "    # ordered in same way for both photo and sketch domains in distance matrix  #\n",
    "    # that is passed to visualize matrix below.                                 #\n",
    "    #############################################################################        \n",
    "    \n",
    "    if viz==True:\n",
    "        if obj_list is not None:\n",
    "            visualize_matrix(D,obj_list=obj_list)\n",
    "        else:\n",
    "            visualize_matrix(D)\n",
    "    return D\n",
    "\n",
    "def get_upper_triangle(D):\n",
    "    #############################################################################\n",
    "    # TODO: Return only values (inds) in upper triangle of square matrix.       #\n",
    "    #############################################################################\n",
    "    return D[inds]\n",
    "\n",
    "def evaluate_rdm_similarity(D1,D2):\n",
    "    '''\n",
    "    input: two distance matrices\n",
    "    output: r, Spearman rank correlation between values in upper-triangle of these two vertices\n",
    "    '''\n",
    "    #############################################################################\n",
    "    # TODO: Fill in function according to docstring, using get_upper_triangle.  #\n",
    "    #############################################################################        \n",
    "    return r "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore \"FC6\" feature representation of matched photos and sketches "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load in features and metadata\n",
    "PF,PM = load_features(feature_dir,layer_name='FC6',data_type='photo') ## photos\n",
    "SF,SM = load_features(feature_dir,layer_name='FC6',data_type='sketch') ## sketches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## concatenate feature matrix and metadata along columns\n",
    "P = pd.concat([pd.DataFrame(PF),PM],axis=1)\n",
    "S = pd.concat([pd.DataFrame(SF),SM],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get category-mean feature vectors for each image domain\n",
    "#############################################################################\n",
    "# TODO: Using groupby from pandas, compute mean feature vectors for each    #\n",
    "# category for P and S, and assign to variables: Pmean & Smean, resp.       #\n",
    "#############################################################################\n",
    "Pmean = P##\n",
    "Smean = S##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get common cluster inds, based on clustering applied to one of these image domains\n",
    "Pmean, Smean = get_common_cluster_inds(Pmean,Smean,n_clusters=14)\n",
    "#############################################################################\n",
    "# TODO: Play around with different values of n_clusters                     #\n",
    "#############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## get ordered distance matrix (ordered on common cluster inds)\n",
    "PD = get_ordered_distance_matrix(Pmean,viz=True,metric='correlation')\n",
    "SD = get_ordered_distance_matrix(Smean,viz=True,metric='correlation')\n",
    "#############################################################################\n",
    "# TODO: Play around with different choices of distance metric.              #\n",
    "#############################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'PD' in locals():\n",
    "    r = evaluate_rdm_similarity(PD,SD)\n",
    "    print('FC6 Similarity between Photo and Sketch Domans = ', np.round(r,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generalize to other layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_layer_append_result(R=None,layer_name='FC6',viz=False):\n",
    "    '''\n",
    "    input: R = dictionary to store results of cross-domain similarity analysis. \n",
    "               keys are layer names, values are RDM similarities            \n",
    "           layer_name = string in ['P1','P2','P3','P4','P5','FC6','FC7']\n",
    "           viz = boolean flag to control whether to display matrices or not\n",
    "    output: R = same dictionary with additional layer result appended    \n",
    "    '''\n",
    "\n",
    "    #############################################################################\n",
    "    # TODO: Fill in this function with the necessary steps to apply all of the  #\n",
    "    # analysis steps from above to an arbitrary layer, by name. These steps     #\n",
    "    # should yield two ordered distance matrices, to be passed into             #\n",
    "    # evaluate_rdm_similarity.                                                  #    \n",
    "    #############################################################################\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    \n",
    "    ## load in features and metadata\n",
    "\n",
    "    ## concatenate feature matrix and metadata along columns\n",
    "\n",
    "    ## get category-mean feature vectors for each image domain\n",
    "\n",
    "    ## get common cluster inds, based on clustering applied to one of these image domains\n",
    "\n",
    "    ## get ordered distance matrix (ordered on common cluster inds)\n",
    "    pass\n",
    "\n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****        \n",
    "        \n",
    "    r = evaluate_rdm_similarity(PD,SD)\n",
    "    if R is None:\n",
    "        R = dict()\n",
    "    R[layer_name] = r    \n",
    "    return R\n",
    "\n",
    "\n",
    "def analyze_all_layers(R=None,layer_list=['P1','P2','P3','P4','P5','FC6','FC7']):\n",
    "    '''\n",
    "    iterate over all layers, calling func analyze_layer_append_result\n",
    "    '''\n",
    "    if R is None:\n",
    "        R = dict()\n",
    "    for i,layer_name in enumerate(layer_list):\n",
    "        print('Analyzing layer {} ...'.format(layer_name))\n",
    "        R = analyze_layer_append_result(R,layer_name=layer_name,viz=False)\n",
    "        clear_output(wait=True)\n",
    "    return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = analyze_all_layers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bundle cross-domain similarity numbers into dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "# TODO: Convert R into a dataframe called SIM that has the following        #\n",
    "# columns: `layer` and `similarity`, where the similarity values are the    #\n",
    "# correlation between photo-sketch RDMs for each layer of VGG.              #\n",
    "#############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## inspect SIM\n",
    "if 'SIM' in locals():\n",
    "    SIM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "# TODO: Generate lineplot of cross-domain RDM similarity by number using    #\n",
    "# seaborn.lineplot. Make sure that all of your axes are labeled and are     #\n",
    "# scaled appropriately. Save figure out as a PNG/PDF image.                 #\n",
    "#############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
